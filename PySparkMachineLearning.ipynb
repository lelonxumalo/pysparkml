{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning using PySpark\n",
    "\n",
    "This notebook illustrates classification and regression ML libraries in Spark with the help of open source datasets from the UCI Machine Learning Repository. Iris dataset (https://archive.ics.uci.edu/ml/machine-learning-databases/iris/) and power plant data (https://archive.ics.uci.edu/ml/machine-learning-databases/00294/)\n",
    "\n",
    "- A. Classification: (we build ML classifier models for species of the iris plant)\n",
    "    - Naive Bayes\n",
    "    - Multilayer Perceptron\n",
    "    - Decision Trees\n",
    "- B. Regression: (we build ML regression models to predict power generated at a plant using defining features)\n",
    "    - Linear Regression\n",
    "    - Decision Tree Regression\n",
    "    - Gradient-boosted Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Spark Context and Import Spark ML libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1fc63c09a58>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, class='Iris-setosa')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = spark.read.csv(\"iris.csv\", inferSchema=True, header=True)\n",
    "iris_df.take(1) #to see the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### *1. Create Vector Assembler*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, class='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
    "viris_df = vectorAssembler.transform(iris_df)\n",
    "viris_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2. Use String Indexer*\n",
    "- Convert the label names into numeric values for quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|         features|label|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "iviris_df = indexer.fit(viris_df).transform(viris_df)\n",
    "iviris_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms\n",
    "\n",
    "### *1. Naive Bayes Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our dataframe into training and test data. We will do a 60:40 split for training and test data, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = iviris_df.randomSplit([0.6,0.4],1)\n",
    "train_df = splits[0] #corresponding to the first dataframe in splits\n",
    "test_df = splits[1] #corresponding to the second dataframe in splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count() #counts the training instances (92 records)\n",
    "test_df.count() #counts the test instances (58 records)\n",
    "iviris_df.count() #total (150 records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's create our Naive Bayes classifier\n",
    "nb = NaiveBayes(modelType=\"multinomial\") #there are more than 2 different classes in our dataset\n",
    "nbmodel =nb.fit(train_df) #fit the trainin data to our NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=4.5, sepal_width=2.3, petal_length=1.3, petal_width=0.3, class='Iris-setosa', features=DenseVector([4.5, 2.3, 1.3, 0.3]), label=0.0, rawPrediction=DenseVector([-10.3605, -11.0141, -11.7112]), probability=DenseVector([0.562, 0.2924, 0.1456]), prediction=0.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make predictions using the model we just created\n",
    "predictions_df = nbmodel.transform(test_df)\n",
    "predictions_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted class for the instance is 0, which is an index value for one of the species/classes. Next we conduct more thorough evaluation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5862068965517241"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nbaccuracy = evaluator.evaluate(predictions_df)\n",
    "nbaccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is about 59%, which isnt great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms\n",
    "\n",
    "### *2. Multilayer Perceptron Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9482758620689655"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "layers = [4,5,5,3]\n",
    "#the first number refers to inputs (features), and the last refers to the 3 classes of iris species in our labels\n",
    "mlp = MultilayerPerceptronClassifier(layers=layers,seed=1)\n",
    "mlp_model = mlp.fit(train_df)\n",
    "mlp_predictions = mlp_model.transform(test_df)\n",
    "mlp_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is much higher than under NB! approx 95% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms\n",
    "\n",
    "### *3. Decision Tree Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9310344827586207"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\",featuresCol=\"features\" )\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
    "dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is similarly high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use UCI Machine Learning database as we did with classification. See https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant\n",
    "The dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant.\n",
    "- we want to use this data to predict how much power a plat can generate based on defining factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models\n",
    "\n",
    "### *1. Linear Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AT: double, V: double, AP: double, RH: double, PE: double]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "pp_df = spark.read.csv(\"power_plant.csv\", header=True, inferSchema=True)\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=23.25, V=71.29, AP=1008.05, RH=71.36, PE=442.21, features=DenseVector([23.25, 71.29, 1008.05, 71.36]))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"AT\", \"V\", \"AP\", \"RH\"], outputCol=\"features\")\n",
    "#create a new, vectorized, dataframe and call it vpp_df\n",
    "vpp_df=vectorAssembler.transform(pp_df)\n",
    "#look at one row\n",
    "vpp_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.9775, -0.2339, 0.0621, -0.1581])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LinearRegression(featuresCol=\"features\", labelCol=\"PE\")\n",
    "lr_model = lr.fit(vpp_df)\n",
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454.60927430355474"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.557126016749484"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to about 1% error which is pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.save(\"lr_model\") #save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models\n",
    "\n",
    "### *2. Decision Tree Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=23.25, V=71.29, AP=1008.05, RH=71.36, PE=442.21, features=DenseVector([23.25, 71.29, 1008.05, 71.36]))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_df = spark.read.csv(\"power_plant.csv\", header=True, inferSchema=True)\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"AT\", \"V\", \"AP\", \"RH\"], outputCol=\"features\")\n",
    "vpp_df=vectorAssembler.transform(pp_df)\n",
    "vpp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9568"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data into training and test sets\n",
    "splits = vpp_df.randomSplit([0.75,0.25])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "train_df.count() #7249 instances\n",
    "test_df.count() #2367 instances\n",
    "vpp_df.count() #9568 total instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Create a Decision Tree Object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7736989914438706"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pretty small error, ~1% \n",
    "- Fairly accurate prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models\n",
    "\n",
    "### *3. Gradient-Boosted Tree Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.315434615837233"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBT is better than the other models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
